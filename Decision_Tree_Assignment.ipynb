{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theory"
      ],
      "metadata": {
        "id": "IlCKRdVseuby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        " - A Decision Tree is a supervised machine learning algorithm used for both classification and regression problems.\n",
        "It works like a flowchart — each internal node represents a condition (like a question on a feature), each branch shows the outcome of that condition, and each leaf node gives the final result (class label).\n",
        "\n",
        "In classification, it divides the dataset into smaller groups based on the best feature split until the data becomes as pure as possible (all belonging to one class).\n",
        "It uses impurity measures like Gini or Entropy to decide which feature to split on."
      ],
      "metadata": {
        "id": "CXy0Etnxex5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        " - Both Gini and Entropy tell how mixed the classes are in a node.\n",
        "\n",
        "~ Entropy measures randomness or uncertainty.\n",
        "Formula: Entropy = -∑ pᵢ * log₂(pᵢ)\n",
        "\n",
        " ~ Gini Impurity measures how often a randomly chosen element would be incorrectly labeled.\n",
        "Formula: Gini = 1 - ∑ pᵢ²\n",
        "\n",
        "Lower values mean purer data.\n",
        "Decision Tree chooses the feature that gives the highest reduction in impurity — that is, the split that makes the child nodes most pure.\n",
        "In short, both work similarly; Gini is faster, Entropy is more detailed."
      ],
      "metadata": {
        "id": "JC26y2gje7Jm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        " -  ~ Pre-Pruning: You stop growing the tree early by setting limits like max_depth, min_samples_split, etc.\n",
        "➤ Advantage: avoids overfitting and makes training faster.\n",
        "\n",
        " ~ Post-Pruning: You first build a full tree and then remove unnecessary branches.\n",
        "➤ Advantage: keeps useful complexity and improves test accuracy."
      ],
      "metadata": {
        "id": "GoNcnyk1fvVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        " - Information Gain tells how much a feature helps in reducing uncertainty (entropy).\n",
        "Formula:\n",
        "IG = Entropy(parent) - Weighted Avg(Entropy(children))\n",
        "\n",
        "Higher Information Gain means better feature for splitting.\n",
        "So the algorithm chooses the feature with maximum IG to get more accurate and pure child nodes."
      ],
      "metadata": {
        "id": "oKhH-sWTgDyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        " -  ~ Applications:\n",
        "\n",
        "Loan approval / credit risk prediction\n",
        "\n",
        "Disease diagnosis\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Marketing and sales forecasting\n",
        "\n",
        " ~ Advantages:\n",
        "\n",
        "Simple and easy to explain\n",
        "\n",
        "Works on both numeric and categorical data\n",
        "\n",
        "No need for feature scaling\n",
        "\n",
        " ~ Limitations:\n",
        "\n",
        "Can overfit on training data\n",
        "\n",
        "Sensitive to small data changes\n",
        "\n",
        "Less accurate than ensemble methods like Random Forest"
      ],
      "metadata": {
        "id": "kw20RsDugMMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "ESgr9F3qgltQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6 Write a Python program to:\n",
        "#     Load the Iris Dataset\n",
        "#     Train a Decision Tree Classifier using the Gini criterion\n",
        "#     Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "id": "uvT4CfW5gpEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a060c079-9139-4ca2-81b1-2a626d5f0da8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9333333333333333\n",
            "Feature Importances: [0.         0.02857143 0.54117647 0.4302521 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7 Write a Python program to train a Decision Tree Classifier with max_depth=3 and compare accuracy with a fully-grown tree.\n",
        "\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
        "\n",
        "clf_md3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_md3.fit(X_train, y_train)\n",
        "acc_md3 = accuracy_score(y_test, clf_md3.predict(X_test))\n",
        "\n",
        "print(\"Fully-grown Accuracy:\", acc_full)\n",
        "print(\"max_depth=3 Accuracy:\", acc_md3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esIOYg6HhYLB",
        "outputId": "848abaad-351f-4783-edff-7a8ff8f97eea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown Accuracy: 0.9333333333333333\n",
            "max_depth=3 Accuracy: 0.9777777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8 Write a Python program to train a Decision Tree Regressor on the Boston Housing Dataset and print the MSE and feature importances.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Top Feature Importances:\", sorted(zip(data.feature_names, reg.feature_importances_), key=lambda x:x[1], reverse=True)[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UieeOfCFhima",
        "outputId": "a0b9a5d2-f6fc-4632-bcf1-6fbc0801eb62"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.495235205629094\n",
            "Top Feature Importances: [('MedInc', np.float64(0.5285090936963706)), ('AveOccup', np.float64(0.13083767753210346)), ('Latitude', np.float64(0.09371656401749287)), ('Longitude', np.float64(0.08290202505986989)), ('AveRooms', np.float64(0.05297496833123543))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9 Write a Python program to tune Decision Tree hyperparameters using GridSearchCV.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best CV Accuracy:\", grid.best_score_)\n",
        "\n",
        "best_model =_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewMdVElKhuiw",
        "outputId": "30863418-4db4-4c58-8a90-c3b2227987e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Best CV Accuracy: 0.9523809523809523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Healthcare Use Case — Predicting Disease Using Decision Trees\n",
        "\n",
        " - Steps I would follow:\n",
        "\n",
        "1. Check data, find missing values.\n",
        "\n",
        "2. Fill missing numeric values with median, categorical with mode or “Unknown”.\n",
        "\n",
        "3. Encode categorical data (OneHotEncoder or LabelEncoder).\n",
        "\n",
        "4. Train DecisionTreeClassifier on the cleaned data.\n",
        "\n",
        "5. Tune parameters like max_depth and min_samples_split using GridSearchCV.\n",
        "\n",
        "6. Evaluate using accuracy, recall, F1-score, and confusion matrix.\n",
        "\n",
        "Business value:\n",
        "Helps hospitals predict diseases early, prioritize patients, reduce cost and save time by automating initial diagnosis steps."
      ],
      "metadata": {
        "id": "fNovthWhi1gy"
      }
    }
  ]
}